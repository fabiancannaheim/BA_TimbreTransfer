{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d998895f-1973-4b7a-9834-fce3b5ebef27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 16:04:04.839282: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-06 16:04:05.331736: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-03-06 16:04:05.331776: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-03-06 16:04:05.331782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import ddsp.training\n",
    "import tensorboard as tb\n",
    "import tensorflow as tf\n",
    "from ddsp import core\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e2fd2a-86dc-4768-afee-575a26917c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 16:04:07.363099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-06 16:04:07.366808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-06 16:04:07.366909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "#tf.data.experimental.enable_debug_mode()\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b20d52-bbd9-4b80-930c-1207ad482efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/audio/adel\n",
      "../../data/tfrecords/adel\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'cello_div' # enter your model name\n",
    "DATASET_NAME = 'cello_div' # enter your dataset name\n",
    "NUM_SHARDS = 10\n",
    "SAMPLE_RATE = 16000\n",
    "#AUDIO_DIR = '../../data/' + DATASET_NAME + '/audio'\n",
    "AUDIO_DIR = '../../data/audio/' + DATASET_NAME\n",
    "AUDIO_FILEPATTERN = AUDIO_DIR + '/*'\n",
    "#SAVE_DIR = '../../models/' + MODEL_NAME\n",
    "SAVE_DIR = '../../data/tfrecords/' + MODEL_NAME\n",
    "TRAIN_TFRECORD = SAVE_DIR + '/train.tfrecord'\n",
    "TRAIN_TFRECORD_FILEPATTERN = TRAIN_TFRECORD + '*'\n",
    "print(AUDIO_DIR)\n",
    "print(SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eaf84fd-49ae-4f6c-a51d-78b479582073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_power(audio,\n",
    "                  sample_rate=16000,\n",
    "                  frame_rate=250,\n",
    "                  frame_size=512,\n",
    "                  ref_db=0.0,\n",
    "                  range_db=core.DB_RANGE,\n",
    "                  padding='center'):\n",
    "  \"\"\"Compute power of audio in dB.\"\"\"\n",
    "  rms_energy = compute_rms_energy(\n",
    "      audio, sample_rate, frame_rate, frame_size, padding=padding)\n",
    "  power_db = core.amplitude_to_db(\n",
    "      rms_energy, ref_db=ref_db, range_db=range_db, use_tf=True)\n",
    "  return power_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687b2b53-aa5c-4d8b-915e-fcbd5dfe8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rms_energy(audio,\n",
    "                       sample_rate=16000,\n",
    "                       frame_rate=250,\n",
    "                       frame_size=512,\n",
    "                       padding='center'):\n",
    "  \"\"\"Compute root mean squared energy of audio.\"\"\"\n",
    "  audio = core.tf_float32(audio)\n",
    "  hop_size = sample_rate // frame_rate\n",
    "  audio = pad(audio, frame_size, hop_size, padding=padding)\n",
    "  audio_frames = tf.signal.frame(audio, frame_size, hop_size, pad_end=False)\n",
    "  rms_energy = tf.reduce_mean(audio_frames**2.0, axis=-1)**0.5\n",
    "  return rms_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c307d9-b90f-4d9c-aea6-db5be13d9cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, frame_size, hop_size, padding='center',\n",
    "        axis=1, mode='CONSTANT', constant_values=0):\n",
    "  \"\"\"Pad a tensor for strided framing such as tf.signal.frame.\n",
    "\n",
    "  Args:\n",
    "    x: Tensor to pad, any shape.\n",
    "    frame_size: Size of frames for striding.\n",
    "    hop_size: Striding, space between frames.\n",
    "    padding: Type of padding to apply, ['valid', 'same', 'center']. 'valid' is\n",
    "      a no-op. 'same' applies padding to the end such that\n",
    "      n_frames = n_t / hop_size. 'center' applies padding to both ends such that\n",
    "      each frame timestamp is centered and n_frames = n_t / hop_size + 1.\n",
    "    axis: Axis along which to pad `x`.\n",
    "    mode: Padding mode for tf.pad(). One of \"CONSTANT\", \"REFLECT\", or\n",
    "      \"SYMMETRIC\" (case-insensitive).\n",
    "    constant_values: Passthrough kwarg for tf.pad().\n",
    "\n",
    "  Returns:\n",
    "    A padded version of `x` along axis. Output sizes can be computed separately\n",
    "      with strided_lengths.\n",
    "  \"\"\"\n",
    "  x = core.tf_float32(x)\n",
    "\n",
    "  if padding == 'valid':\n",
    "    return x\n",
    "\n",
    "  if hop_size > frame_size:\n",
    "    raise ValueError(f'During padding, frame_size ({frame_size})'\n",
    "                     f' must be greater than hop_size ({hop_size}).')\n",
    "\n",
    "  if len(x.shape) <= 1:\n",
    "    axis = 0\n",
    "\n",
    "  n_t = x.shape[axis]\n",
    "  _, n_t_padded = get_framed_lengths(n_t, frame_size, hop_size, padding)\n",
    "  pads = [[0, 0] for _ in range(len(x.shape))]\n",
    "\n",
    "  if padding == 'same':\n",
    "    pad_amount = int(n_t_padded - n_t)\n",
    "    pads[axis] = [0, pad_amount]\n",
    "\n",
    "  elif padding == 'center':\n",
    "    pad_amount = int(frame_size // 2)  # Symmetric even padding like librosa.\n",
    "    pads[axis] = [pad_amount, pad_amount]\n",
    "\n",
    "  else:\n",
    "    raise ValueError('`padding` must be one of [\\'center\\', \\'same\\''\n",
    "                     f'\\'valid\\'], received ({padding}).')\n",
    "\n",
    "  return tf.pad(x, pads, mode=mode, constant_values=constant_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39d82814-cdd1-4573-9d83-62c950cd2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_framed_lengths(input_length, frame_size, hop_size, padding='center'):\n",
    "  \"\"\"Give a strided framing, such as tf.signal.frame, gives output lengths.\n",
    "\n",
    "  Args:\n",
    "    input_length: Original length along the dimension to be framed.\n",
    "    frame_size: Size of frames for striding.\n",
    "    hop_size: Striding, space between frames.\n",
    "    padding: Type of padding to apply, ['valid', 'same', 'center']. 'valid' is\n",
    "      a no-op. 'same' applies padding to the end such that\n",
    "      n_frames = n_t / hop_size. 'center' applies padding to both ends such that\n",
    "      each frame timestamp is centered and n_frames = n_t / hop_size + 1.\n",
    "\n",
    "  Returns:\n",
    "    n_frames: Number of frames left after striding.\n",
    "    padded_length: Length of the padded signal before striding.\n",
    "  \"\"\"\n",
    "  # Use numpy since this function isn't used dynamically.\n",
    "  def get_n_frames(length):\n",
    "    return int(np.floor((length - frame_size) / hop_size)) + 1\n",
    "\n",
    "  if padding == 'valid':\n",
    "    padded_length = input_length\n",
    "    n_frames = get_n_frames(input_length)\n",
    "\n",
    "  elif padding == 'center':\n",
    "    padded_length = input_length + frame_size\n",
    "    n_frames = get_n_frames(padded_length)\n",
    "\n",
    "  elif padding == 'same':\n",
    "    n_frames = int(np.ceil(input_length / hop_size))\n",
    "    padded_length = (n_frames - 1) * hop_size + frame_size\n",
    "\n",
    "  return n_frames, padded_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09346347-8d31-46fa-b0aa-938120ae0393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_notes(loudness_db,\n",
    "                 f0_confidence,\n",
    "                 note_threshold=1.0,\n",
    "                 exponent=2.0,\n",
    "                 smoothing=40,\n",
    "                 f0_confidence_threshold=0.7,\n",
    "                 min_db=-core.DB_RANGE):\n",
    "  \"\"\"Detect note on-off using loudness and smoothed f0_confidence.\"\"\"\n",
    "  mean_db = np.mean(loudness_db)\n",
    "  db = smooth(f0_confidence**exponent, smoothing) * (loudness_db - min_db)\n",
    "  db_threshold = (mean_db - min_db) * f0_confidence_threshold**exponent\n",
    "  note_on_ratio = db / db_threshold\n",
    "  mask_on = note_on_ratio >= note_threshold\n",
    "  return mask_on, note_on_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbd89c7c-0e53-4cad-a6f7-b0e3c54e42c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x, filter_size=3):\n",
    "  \"\"\"Smooth 1-d signal with a box filter.\"\"\"\n",
    "  x = tf.convert_to_tensor(x, tf.float32)\n",
    "  is_2d = len(x.shape) == 2\n",
    "  x = x[:, :, tf.newaxis] if is_2d else x[tf.newaxis, :, tf.newaxis]\n",
    "  w = tf.ones([filter_size])[:, tf.newaxis, tf.newaxis] / float(filter_size)\n",
    "  y = tf.nn.conv1d(x, w, stride=1, padding='SAME')\n",
    "  y = y[:, :, 0] if is_2d else y[0, :, 0]\n",
    "  return y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5474af67-c3dc-4669-92b1-81dc785b5415",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileTransformer:\n",
    "  \"\"\"Transform features using quantiles information.\n",
    "\n",
    "  Stripped down version of sklearn.preprocessing.QuantileTransformer.\n",
    "  https://github.com/scikit-learn/scikit-learn/blob/\n",
    "  863e58fcd5ce960b4af60362b44d4f33f08c0f97/sklearn/preprocessing/_data.py\n",
    "\n",
    "  Putting directly in ddsp library to avoid dependency on sklearn that breaks\n",
    "  when pickling and unpickling from different versions of sklearn.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               n_quantiles=1000,\n",
    "               output_distribution='uniform',\n",
    "               subsample=int(1e5)):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      n_quantiles: int, default=1000 or n_samples Number of quantiles to be\n",
    "        computed. It corresponds to the number of landmarks used to discretize\n",
    "        the cumulative distribution function. If n_quantiles is larger than the\n",
    "        number of samples, n_quantiles is set to the number of samples as a\n",
    "        larger number of quantiles does not give a better approximation of the\n",
    "        cumulative distribution function estimator.\n",
    "      output_distribution: {'uniform', 'normal'}, default='uniform' Marginal\n",
    "        distribution for the transformed data. The choices are 'uniform'\n",
    "        (default) or 'normal'.\n",
    "      subsample: int, default=1e5 Maximum number of samples used to estimate\n",
    "        the quantiles for computational efficiency. Note that the subsampling\n",
    "        procedure may differ for value-identical sparse and dense matrices.\n",
    "    \"\"\"\n",
    "    self.n_quantiles = n_quantiles\n",
    "    self.output_distribution = output_distribution\n",
    "    self.subsample = subsample\n",
    "    self.random_state = np.random.mtrand._rand\n",
    "\n",
    "  def _dense_fit(self, x, random_state):\n",
    "    \"\"\"Compute percentiles for dense matrices.\n",
    "\n",
    "    Args:\n",
    "      x: ndarray of shape (n_samples, n_features)\n",
    "        The data used to scale along the features axis.\n",
    "      random_state: Numpy random number generator.\n",
    "    \"\"\"\n",
    "    n_samples, _ = x.shape\n",
    "    references = self.references_ * 100\n",
    "\n",
    "    self.quantiles_ = []\n",
    "    for col in x.T:\n",
    "      if self.subsample < n_samples:\n",
    "        subsample_idx = random_state.choice(\n",
    "            n_samples, size=self.subsample, replace=False)\n",
    "        col = col.take(subsample_idx, mode='clip')\n",
    "      self.quantiles_.append(np.nanpercentile(col, references))\n",
    "    self.quantiles_ = np.transpose(self.quantiles_)\n",
    "    # Due to floating-point precision error in `np.nanpercentile`,\n",
    "    # make sure that quantiles are monotonically increasing.\n",
    "    # Upstream issue in numpy:\n",
    "    # https://github.com/numpy/numpy/issues/14685\n",
    "    self.quantiles_ = np.maximum.accumulate(self.quantiles_)\n",
    "\n",
    "  def fit(self, x):\n",
    "    \"\"\"Compute the quantiles used for transforming.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Args:\n",
    "      x: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "        The data used to scale along the features axis. If a sparse\n",
    "        matrix is provided, it will be converted into a sparse\n",
    "        ``csc_matrix``. Additionally, the sparse matrix needs to be\n",
    "        nonnegative if `ignore_implicit_zeros` is False.\n",
    "\n",
    "    Returns:\n",
    "      self: object\n",
    "         Fitted transformer.\n",
    "    \"\"\"\n",
    "    if self.n_quantiles <= 0:\n",
    "      raise ValueError(\"Invalid value for 'n_quantiles': %d. \"\n",
    "                       'The number of quantiles must be at least one.' %\n",
    "                       self.n_quantiles)\n",
    "    n_samples = x.shape[0]\n",
    "    self.n_quantiles_ = max(1, min(self.n_quantiles, n_samples))\n",
    "\n",
    "    # Create the quantiles of reference\n",
    "    self.references_ = np.linspace(0, 1, self.n_quantiles_, endpoint=True)\n",
    "    self._dense_fit(x, self.random_state)\n",
    "    return self\n",
    "\n",
    "  def _transform_col(self, x_col, quantiles, inverse):\n",
    "    \"\"\"Private function to transform a single feature.\"\"\"\n",
    "    output_distribution = self.output_distribution\n",
    "    bounds_threshold = 1e-7\n",
    "\n",
    "    if not inverse:\n",
    "      lower_bound_x = quantiles[0]\n",
    "      upper_bound_x = quantiles[-1]\n",
    "      lower_bound_y = 0\n",
    "      upper_bound_y = 1\n",
    "    else:\n",
    "      lower_bound_x = 0\n",
    "      upper_bound_x = 1\n",
    "      lower_bound_y = quantiles[0]\n",
    "      upper_bound_y = quantiles[-1]\n",
    "      # for inverse transform, match a uniform distribution\n",
    "      with np.errstate(invalid='ignore'):  # hide NaN comparison warnings\n",
    "        if output_distribution == 'normal':\n",
    "          x_col = stats.norm.cdf(x_col)\n",
    "        # else output distribution is already a uniform distribution\n",
    "\n",
    "    # find index for lower and higher bounds\n",
    "    with np.errstate(invalid='ignore'):  # hide NaN comparison warnings\n",
    "      if output_distribution == 'normal':\n",
    "        lower_bounds_idx = (x_col - bounds_threshold < lower_bound_x)\n",
    "        upper_bounds_idx = (x_col + bounds_threshold > upper_bound_x)\n",
    "      if output_distribution == 'uniform':\n",
    "        lower_bounds_idx = (x_col == lower_bound_x)\n",
    "        upper_bounds_idx = (x_col == upper_bound_x)\n",
    "\n",
    "    isfinite_mask = ~np.isnan(x_col)\n",
    "    x_col_finite = x_col[isfinite_mask]\n",
    "    if not inverse:\n",
    "      # Interpolate in one direction and in the other and take the\n",
    "      # mean. This is in case of repeated values in the features\n",
    "      # and hence repeated quantiles\n",
    "      #\n",
    "      # If we don't do this, only one extreme of the duplicated is\n",
    "      # used (the upper when we do ascending, and the\n",
    "      # lower for descending). We take the mean of these two\n",
    "      x_col[isfinite_mask] = .5 * (\n",
    "          np.interp(x_col_finite, quantiles, self.references_) -\n",
    "          np.interp(-x_col_finite, -quantiles[::-1], -self.references_[::-1]))\n",
    "    else:\n",
    "      x_col[isfinite_mask] = np.interp(x_col_finite, self.references_,\n",
    "                                       quantiles)\n",
    "\n",
    "    x_col[upper_bounds_idx] = upper_bound_y\n",
    "    x_col[lower_bounds_idx] = lower_bound_y\n",
    "    # for forward transform, match the output distribution\n",
    "    if not inverse:\n",
    "      with np.errstate(invalid='ignore'):  # hide NaN comparison warnings\n",
    "        if output_distribution == 'normal':\n",
    "          x_col = stats.norm.ppf(x_col)\n",
    "          # find the value to clip the data to avoid mapping to\n",
    "          # infinity. Clip such that the inverse transform will be\n",
    "          # consistent\n",
    "          clip_min = stats.norm.ppf(bounds_threshold - np.spacing(1))\n",
    "          clip_max = stats.norm.ppf(1 - (bounds_threshold - np.spacing(1)))\n",
    "          x_col = np.clip(x_col, clip_min, clip_max)\n",
    "        # else output distribution is uniform and the ppf is the\n",
    "        # identity function so we let x_col unchanged\n",
    "\n",
    "    return x_col\n",
    "\n",
    "  def _transform(self, x, inverse=False):\n",
    "    \"\"\"Forward and inverse transform.\n",
    "\n",
    "    Args:\n",
    "      x : ndarray of shape (n_samples, n_features)\n",
    "        The data used to scale along the features axis.\n",
    "      inverse : bool, default=False\n",
    "        If False, apply forward transform. If True, apply\n",
    "        inverse transform.\n",
    "\n",
    "    Returns:\n",
    "      x : ndarray of shape (n_samples, n_features)\n",
    "        Projected data\n",
    "    \"\"\"\n",
    "    x = np.array(x)  # Explicit copy.\n",
    "    for feature_idx in range(x.shape[1]):\n",
    "      x[:, feature_idx] = self._transform_col(\n",
    "          x[:, feature_idx], self.quantiles_[:, feature_idx], inverse)\n",
    "    return x\n",
    "\n",
    "  def transform(self, x):\n",
    "    \"\"\"Feature-wise transformation of the data.\"\"\"\n",
    "    return self._transform(x, inverse=False)\n",
    "\n",
    "  def inverse_transform(self, x):\n",
    "    \"\"\"Back-projection to the original space.\"\"\"\n",
    "    return self._transform(x, inverse=True)\n",
    "\n",
    "  def fit_transform(self, x):\n",
    "    \"\"\"Fit and transform.\"\"\"\n",
    "    return self.fit(x).transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bc35d0d-ad75-4762-96c3-bc08f235dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_quantile_transform(loudness_db, mask_on, inv_quantile=None):\n",
    "  \"\"\"Fits quantile normalization, given a note_on mask.\n",
    "\n",
    "  Optionally, performs the inverse transformation given a pre-fitted transform.\n",
    "  Args:\n",
    "    loudness_db: Decibels, shape [batch, time]\n",
    "    mask_on: A binary mask for when a note is present, shape [batch, time].\n",
    "    inv_quantile: Optional pretrained QuantileTransformer to perform the inverse\n",
    "      transformation.\n",
    "\n",
    "  Returns:\n",
    "    Trained quantile transform. Also returns the renormalized loudnesses if\n",
    "      inv_quantile is provided.\n",
    "  \"\"\"\n",
    "  quantile_transform = QuantileTransformer()\n",
    "  loudness_flat = np.ravel(loudness_db[mask_on])[:, np.newaxis]\n",
    "  loudness_flat_q = quantile_transform.fit_transform(loudness_flat)\n",
    "\n",
    "  if inv_quantile is None:\n",
    "    return quantile_transform\n",
    "  else:\n",
    "    loudness_flat_norm = inv_quantile.inverse_transform(loudness_flat_q)\n",
    "    loudness_norm = np.ravel(loudness_db.copy())[:, np.newaxis]\n",
    "    loudness_norm[mask_on] = loudness_flat_norm\n",
    "    return quantile_transform, loudness_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4112041-417e-44a0-baf5-479e547e6815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 16:04:15.398780: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-06 16:04:15.399801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-06 16:04:15.399964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-06 16:04:15.400051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-06 16:04:15.733652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-06 16:04:15.733790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-06 16:04:15.733882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-06 16:04:15.733950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_provider = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN, centered=True) # Set 'centered = False' if you are not using center paddding \n",
    "dataset = data_provider.get_dataset(shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a05f043a-38b4-4e9c-ba5d-d9674bc8e0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating dataset statistics for <ddsp.training.data.TFRecordProvider object at 0x7f81563c8b50>\n",
      "Computing statistics for 3072 examples.\n"
     ]
    }
   ],
   "source": [
    "print('Calculating dataset statistics for', data_provider)\n",
    "\n",
    "batch_size= 16\n",
    "power_frame_size = 512\n",
    "power_frame_rate = 250\n",
    "\n",
    "\n",
    "\n",
    "ds = data_provider.get_batch(batch_size, repeats=1)\n",
    "\n",
    "i = 0\n",
    "loudness = []\n",
    "power = []\n",
    "f0 = []\n",
    "f0_conf = []\n",
    "audio = []\n",
    "\n",
    "batch = next(iter(ds))\n",
    "audio_key = 'audio_16k' if 'audio_16k' in batch.keys() else 'audio'\n",
    "\n",
    "for batch in iter(ds):\n",
    "    loudness.append(batch['loudness_db'])\n",
    "    power.append(\n",
    "        compute_power(batch[audio_key],\n",
    "                       frame_size=power_frame_size,\n",
    "                       frame_rate=power_frame_rate))\n",
    "    f0.append(batch['f0_hz'])\n",
    "    f0_conf.append(batch['f0_confidence'])\n",
    "    audio.append(batch[audio_key])\n",
    "    i += 1\n",
    "\n",
    "print(f'Computing statistics for {i * batch_size} examples.')\n",
    "\n",
    "loudness = np.vstack(loudness)\n",
    "power = np.vstack(power)\n",
    "f0 = np.vstack(f0)\n",
    "f0_conf = np.vstack(f0_conf)\n",
    "audio = np.vstack(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09d0e9b6-72b9-46a3-9047-56f71240b948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 1001)\n",
      "(3072, 1001)\n",
      "(3072, 1001)\n",
      "(3072, 1001)\n",
      "(3072, 64000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(loudness.shape)\n",
    "print(power.shape)\n",
    "print(f0.shape)\n",
    "print(f0_conf.shape)\n",
    "print(audio.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac656d1-6692-4bbe-9915-475962143b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 16:04:45.526169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "../../models/MODEL_NAME/dataset_statistics.plk; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PICKLE_FILE_PATH \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(PICKLE_FILE_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 61\u001b[0m       \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone! Saved dataset statistics to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPICKLE_FILE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py:100\u001b[0m, in \u001b[0;36mFileIO.write\u001b[0;34m(self, file_content)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_content):\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Writes file_content to the file. Appends to the end of the file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prewrite_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writable_file\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    102\u001b[0m       compat\u001b[38;5;241m.\u001b[39mas_bytes(file_content, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__encoding))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py:85\u001b[0m, in \u001b[0;36mFileIO._prewrite_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_check_passed:\n\u001b[1;32m     83\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mPermissionDeniedError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt open for writing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writable_file \u001b[38;5;241m=\u001b[39m \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWritableFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__mode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ../../models/MODEL_NAME/dataset_statistics.plk; No such file or directory"
     ]
    }
   ],
   "source": [
    "# Fit the transform.\n",
    "trim_end = 20\n",
    "f0_trimmed = f0[:, :-trim_end]\n",
    "pitch_trimmed = core.hz_to_midi(f0_trimmed)\n",
    "power_trimmed = power[:, :-trim_end]\n",
    "loudness_trimmed = loudness[:, :-trim_end]\n",
    "f0_conf_trimmed = f0_conf[:, :-trim_end]\n",
    "\n",
    "# Detect notes.\n",
    "mask_on, _ = detect_notes(loudness_trimmed, f0_conf_trimmed)\n",
    "\n",
    "# If no notes detected, just default to using full signal.\n",
    "mask_on = np.logical_or(\n",
    "  mask_on, np.logical_not(np.any(mask_on, axis=1, keepdims=True)))\n",
    "\n",
    "quantile_transform = fit_quantile_transform(loudness_trimmed, mask_on)\n",
    "\n",
    "# Pitch statistics.\n",
    "def get_stats(x, prefix='x', note_mask=None):\n",
    "    if note_mask is None:\n",
    "      mean_max = np.mean(np.max(x, axis=-1))\n",
    "      mean_min = np.mean(np.min(x, axis=-1))\n",
    "    else:\n",
    "      max_list = []\n",
    "      for x_i, m in zip(x, note_mask):\n",
    "        if np.sum(m) > 0:\n",
    "          max_list.append(np.max(x_i[m]))\n",
    "      mean_max = np.mean(max_list)\n",
    "    \n",
    "      min_list = []\n",
    "      for x_i, m in zip(x, note_mask):\n",
    "        if np.sum(m) > 0:\n",
    "          min_list.append(np.min(x_i[m]))\n",
    "      mean_min = np.mean(min_list)\n",
    "    \n",
    "      x = x[note_mask]\n",
    "    \n",
    "    return {\n",
    "        f'mean_{prefix}': np.mean(x),\n",
    "        f'max_{prefix}': np.max(x),\n",
    "        f'min_{prefix}': np.min(x),\n",
    "        f'mean_max_{prefix}': mean_max,\n",
    "        f'mean_min_{prefix}': mean_min,\n",
    "        f'std_{prefix}': np.std(x)\n",
    "    }\n",
    "\n",
    "ds_stats = {}\n",
    "ds_stats.update(get_stats(pitch_trimmed, 'pitch'))\n",
    "ds_stats.update(get_stats(power_trimmed, 'power'))\n",
    "ds_stats.update(get_stats(loudness_trimmed, 'loudness'))\n",
    "ds_stats.update(get_stats(pitch_trimmed, 'pitch_note', mask_on))\n",
    "ds_stats.update(get_stats(power_trimmed, 'power_note', mask_on))\n",
    "ds_stats.update(get_stats(loudness_trimmed, 'loudness_note', mask_on))\n",
    "\n",
    "ds_stats['quantile_transform'] = quantile_transform\n",
    "\n",
    "FILE_NAME = 'dataset_statistics.plk'\n",
    "PICKLE_FILE_PATH = '../../models/' +MODEL_NAME+ '/' + FILE_NAME\n",
    "if PICKLE_FILE_PATH is not None:\n",
    "    with tf.io.gfile.GFile(PICKLE_FILE_PATH, 'wb') as f:\n",
    "      pickle.dump(ds_stats, f)\n",
    "    print(f'Done! Saved dataset statistics to: {PICKLE_FILE_PATH}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
